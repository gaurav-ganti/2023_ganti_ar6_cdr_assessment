{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "if (typeof IPython !== 'undefined') { IPython.OutputArea.prototype._should_scroll = function(lines){ return false; }}",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read in the compiled CDR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyam - INFO: Running in a notebook, setting up a basic logging at level INFO\n",
      "pyam.core - INFO: Reading file ../data/101_data.xlsx\n",
      "pyam.core - INFO: Reading meta indicators\n"
     ]
    }
   ],
   "source": [
    "df = pyam.IamDataFrame(\n",
    "    Path(\n",
    "        '../data/101_data.xlsx'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Read in the filter sets for land and pe bio that we prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_set_land = pd.read_csv(\n",
    "    Path(\n",
    "        '../data/103_filter_set_land.csv'\n",
    "    ),\n",
    "    index_col=[0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_set_bio = pd.read_csv(\n",
    "    Path(\n",
    "        '../data/103_filter_set_bio.csv'\n",
    "    ),\n",
    "    index_col=[0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Concatenate the two so that we can assign this to the global CDR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_all = pd.concat(\n",
    "    [\n",
    "        filter_set_land,\n",
    "        filter_set_bio\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_all_overlap = (\n",
    "    filter_all\n",
    "    .loc[\n",
    "        df.meta.index.intersection(filter_all.index)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create a new metadata column and assign the two sets of netzero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['exclude', 'land_cover_max', 'land_cover_netzero', 'land_sustainable',\n",
       "       'exclude', 'pe_bio_max', 'pe_bio_netzero', 'bio_sustainable'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_all_overlap.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['land_cover_netzero', 'pe_bio_netzero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cols:\n",
    "    df.set_meta(\n",
    "        meta=filter_all_overlap[c],\n",
    "        name=c\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: For each world region, we now want to calculate the cumulative land and novel CDR over two time periods:\n",
    "* 2020 to global net zero CO2\n",
    "* Global net zero CO2 to 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzero_year = df.meta.loc[\n",
    "    :,\n",
    "    'Year of netzero CO2 emissions (Harm-Infilled) table'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzero_year.fillna(\n",
    "    2100,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_cdr = 'AR6 Reanalysis|OSCARv3.2|Carbon Removal|Non-Land'\n",
    "conventional_cdr = 'AR6 Reanalysis|OSCARv3.2|Carbon Removal|Land'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg in df.region:\n",
    "    if reg == 'World':\n",
    "        continue\n",
    "    for var in [novel_cdr, conventional_cdr]:\n",
    "        if var == novel_cdr:\n",
    "            meta_prefix = 'novel_cdr'\n",
    "        else:\n",
    "            meta_prefix = 'conventional_cdr'\n",
    "        ts_data = (\n",
    "            df\n",
    "            .filter(\n",
    "                region=reg,\n",
    "                variable=var\n",
    "            )\n",
    "            .timeseries()\n",
    "        )\n",
    "        # The cumulative estimate to net zero\n",
    "        df.set_meta(\n",
    "            meta=ts_data.apply(\n",
    "               lambda x: pyam.cumulative(\n",
    "                   x,\n",
    "                   first_year=2020,\n",
    "                   last_year=netzero_year[x.name[0:2]]\n",
    "               ),\n",
    "               axis=1\n",
    "            ),\n",
    "            name=f'{meta_prefix}_{reg}_2020_netzero'\n",
    "        )\n",
    "        # The cumulative estimate to 2100\n",
    "        df.set_meta(\n",
    "            meta=ts_data.apply(\n",
    "                lambda x: pyam.cumulative(\n",
    "                    x,\n",
    "                    first_year=netzero_year[x.name[0:2]],\n",
    "                    last_year=2100\n",
    "                ),\n",
    "                axis=1\n",
    "            ),\n",
    "            name=f'{meta_prefix}_{reg}_netzero_2100'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Now, we want a processing function that takes the following information:\n",
    "* The column to categorise by\n",
    "* Bins\n",
    "* Bin labels\n",
    "\n",
    "And then returns a melted dataframe with the categories that are ready to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "        col,\n",
    "        bins,\n",
    "        bin_labels\n",
    "):\n",
    "    \"\"\"Helper function to process and bin data\"\"\"\n",
    "    df.meta.loc[:, f'category_{col}'] = (\n",
    "        pd.cut(\n",
    "            df.meta.loc[:, col],\n",
    "            bins=bins\n",
    "        )\n",
    "        .map(bin_labels)\n",
    "    )\n",
    "    # Step 6.1: Decide which columns need to be melted.\n",
    "    cols_to_melt = [\n",
    "        x for x in df.meta.columns if 'novel_cdr' in x or 'conventional_cdr' in x\n",
    "    ]\n",
    "    # Step 6.2: Melt the dataframe\n",
    "    melted_data = (\n",
    "        pd.melt(\n",
    "            frame=df.meta.reset_index(),\n",
    "            id_vars=['model', 'scenario', 'Category', f'category_{col}'],\n",
    "            value_vars=cols_to_melt\n",
    "        )\n",
    "    )\n",
    "    # Step 6.3: Split the variable column\n",
    "    melted_data['variable'] = melted_data['variable'].apply(\n",
    "        lambda x: x.replace('cdr', 'cdr_World') if 'R5' not in x else x\n",
    "    )\n",
    "    melted_data.loc[:, 'cdr_type'] = melted_data['variable'].apply(\n",
    "        lambda x: x.split('_')[0] + '_' + x.split('_')[1]\n",
    "    )\n",
    "    melted_data.loc[:, 'region'] = melted_data['variable'].apply(\n",
    "        lambda x: x.split('_')[2]\n",
    "    )\n",
    "    melted_data.loc[:, 'timeframe'] = melted_data['variable'].apply(\n",
    "        lambda x: x.split('_')[3] + '_' + x.split('_')[4]\n",
    "    )\n",
    "    return melted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Let us start with the land area filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_for_land = (\n",
    "    pd\n",
    "    .IntervalIndex\n",
    "    .from_tuples(\n",
    "        [\n",
    "            (0, 100),\n",
    "            (100, 400),\n",
    "            (400, 2000)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "land_labels = dict(\n",
    "    zip(\n",
    "        bins_for_land,\n",
    "        [\n",
    "            '0-100',\n",
    "            '100-400',\n",
    "            '400+'\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "land_estimates = process_data(\n",
    "    col='land_cover_netzero',\n",
    "    bins=bins_for_land,\n",
    "    bin_labels=land_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Now let us move on to the bioenergy demand filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_for_pe = (\n",
    "    pd\n",
    "    .IntervalIndex\n",
    "    .from_tuples(\n",
    "        [\n",
    "            (0, 100),\n",
    "            (100, 200),\n",
    "            (200, 1000)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "pe_labels = dict(\n",
    "    zip(\n",
    "        bins_for_pe,\n",
    "        [\n",
    "            '0-100',\n",
    "            '100-200',\n",
    "            '200+'\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "bio_estimates = process_data(\n",
    "    col='pe_bio_netzero',\n",
    "    bins=bins_for_pe,\n",
    "    bin_labels=pe_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Write this out for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_estimates.to_csv(\n",
    "    Path(\n",
    "        '../data/105_land_categories.csv'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_estimates.to_csv(\n",
    "    Path(\n",
    "        '../data/105_bio_categories.csv'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10L Calculate the global cumulative removals (total) and write this out separately for assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_removals = (\n",
    "    df\n",
    "    .filter(\n",
    "        variable='*Carbon Removal|Total',\n",
    "        region='World'\n",
    "    )\n",
    "    .timeseries()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_meta(\n",
    "    total_removals.apply(\n",
    "        lambda x: pyam.cumulative(\n",
    "            x,\n",
    "            first_year=2020,\n",
    "            last_year=df.meta.loc[x.name[0:2],  'Year of netzero CO2 emissions (Harm-Infilled) table']\n",
    "        ),\n",
    "        axis=1\n",
    "    ),\n",
    "    name='cumulative_removals_2020_netzero'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world_analysis = df.meta[\n",
    "    [\n",
    "        'Category',\n",
    "        'cumulative_removals_2020_netzero',\n",
    "        'category_pe_bio_netzero',\n",
    "        'category_land_cover_netzero'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_world_analysis.to_csv(\n",
    "    Path(\n",
    "        '../data/105_world.csv'\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar6_cdr_assessment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
